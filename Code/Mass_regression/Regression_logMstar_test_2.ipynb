{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../Pre_processing\")\n",
    "\n",
    "from Data_Preparation_Library import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "image_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selectedBatches=[\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"]\n",
    "maxBatchId = 5\n",
    "selectedBatches=[str(i) for i in range(maxBatchId)]\n",
    "\n",
    "extra_folder=str(image_size)+\"_\"+str(image_size)\n",
    "batch_data_object = []\n",
    "for i in selectedBatches:\n",
    "    with open(os.path.join(temp_path,extra_folder,'full_data_object_' + i + '.p'), 'rb') as handle:\n",
    "        batch_data_object+=pickle.load(handle)\n",
    "\n",
    "data_train = batch_data_object[0:int(len(batch_data_object)*2/3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data_test_object = []\n",
    "for i in range(8):\n",
    "    with open(os.path.join(temp_path,extra_folder,'test_data_object_' + str(i) + '.p'), 'rb') as handle:\n",
    "        batch_data_test_object+=pickle.load(handle)\n",
    "        \n",
    "data_test  = batch_data_test_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flux(d):\n",
    "  # TODO: get flux_i from model\n",
    "    #if d.i_image != None:\n",
    "    #    flux_i = sum(sum(d.i_image))\n",
    "    #else:\n",
    "    #    flux_i = 0\n",
    "    flux_g = sum(sum(d.g_image_resized))\n",
    "    #return flux_i+flux_g, flux_g-flux_i\n",
    "    return flux_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8447, (128, 128, 1))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_data_test_object),batch_data_test_object[0].g_image_resized_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "version=\"v1\"\n",
    "res_folder=\"results_\"+str(image_size)+\"_on_\"+str(image_size)\n",
    "with open(os.path.join(os.path.join(output_path,res_folder),\"encoder_results_train_\"+version+\"__all\"),'rb') as handle:\n",
    "    encoded_imgs1=pickle.load(handle)\n",
    "    encoded_imgs_reshaped_train = [e.reshape(-1,1) for e in encoded_imgs1]\n",
    "\n",
    "model_encoder=load_model(os.path.join(os.path.join(output_path,res_folder),\"encoder_model_\"+version+\"__all\"))\n",
    "\n",
    "def get_g_Data_for_Autoencoder(data_object):\n",
    "    X_list=[(data_object[index].g_image_resized_reshaped)/data_object[index].g_image_resized_reshaped.max() for index in range(len(data_object))]\n",
    "    return np.asarray(X_list),np.asarray(X_list)\n",
    "\n",
    "X_test_auto_encoder,y_test_auto_encoder=get_g_Data_for_Autoencoder(batch_data_test_object)\n",
    "\n",
    "encoded_imgs2=model_encoder.predict(X_test_auto_encoder)\n",
    "encoded_imgs_reshaped_test = [e.reshape(-1,1) for e in encoded_imgs2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3277, 4101) (3277,)\n"
     ]
    }
   ],
   "source": [
    "# accessible in the class: size of image, maximum of image, normalization factor of g + fitted value of normalization factor for i\n",
    "# define a function to get the training and testing dataset\n",
    "\n",
    "def obtain_train_data():\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    for i in range(len(data_train)):\n",
    "        if data_train[i].logMstar!=-99:\n",
    "            e = encoded_imgs_reshaped_train[i]\n",
    "            d = data_train[i]\n",
    "            #f1,f2 = flux(d)\n",
    "            f = flux(d)\n",
    "            size_1=d.g_image.shape[0]\n",
    "            size_2=d.g_image.shape[1]\n",
    "            img_max=d.g_image.max()\n",
    "            dist=d.Distance\n",
    "            train_features.append(np.append(e,np.array([f,size_1,size_2,img_max,dist])))\n",
    "            #train_features.append(np.append(e,np.array([f])))\n",
    "            train_labels.append(d.logMstar)\n",
    "    return train_features, train_labels\n",
    "\n",
    "train_features = np.asarray(obtain_train_data()[0])\n",
    "train_labels = np.asarray(obtain_train_data()[1])\n",
    "\n",
    "print(train_features.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8447, 4101) (8447,)\n"
     ]
    }
   ],
   "source": [
    "def obtain_test_data():\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    for j in range(len(data_test)):\n",
    "        if data_test[j].logMstar!=-99:\n",
    "            e = encoded_imgs_reshaped_test[j]\n",
    "            d = data_test[j]\n",
    "\n",
    "        #f1,f2 = flux(d)\n",
    "            f = flux(d)\n",
    "            size_1=d.g_image.shape[0]\n",
    "            size_2=d.g_image.shape[1]\n",
    "            img_max=d.g_image.max()\n",
    "            dist=d.Distance\n",
    "            test_features.append(np.append(e,np.array([f,size_1,size_2,img_max,dist])))\n",
    "            test_labels.append(d.logMstar)\n",
    "\n",
    "    return test_features, test_labels\n",
    "\n",
    "test_features = np.asarray(obtain_test_data()[0])\n",
    "test_labels = np.asarray(obtain_test_data()[1])\n",
    "\n",
    "print(test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## transform features\n",
    "# train_features = StandardScaler().fit_transform(train_features)\n",
    "# test_features = StandardScaler().transform(test_features)\n",
    "# #features = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "train_features = X_scaler.fit_transform(train_features)\n",
    "test_features = X_scaler.transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid', input_dim=train_features.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='Adadelta',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3277 samples, validate on 8447 samples\n",
      "Epoch 1/20\n",
      "3277/3277 [==============================] - 1s - loss: 71.9788 - mean_squared_error: 71.9788 - val_loss: 12.7115 - val_mean_squared_error: 12.7115\n",
      "Epoch 2/20\n",
      "3277/3277 [==============================] - 0s - loss: 37.1849 - mean_squared_error: 37.1849 - val_loss: 29.5600 - val_mean_squared_error: 29.5600\n",
      "Epoch 3/20\n",
      "3277/3277 [==============================] - 0s - loss: 19.8559 - mean_squared_error: 19.8559 - val_loss: 47.3384 - val_mean_squared_error: 47.3384\n",
      "Epoch 4/20\n",
      "3277/3277 [==============================] - 0s - loss: 10.2960 - mean_squared_error: 10.2960 - val_loss: 64.5537 - val_mean_squared_error: 64.5537\n",
      "Epoch 5/20\n",
      "3277/3277 [==============================] - 0s - loss: 4.7746 - mean_squared_error: 4.7746 - val_loss: 82.1469 - val_mean_squared_error: 82.1469\n",
      "Epoch 6/20\n",
      "3277/3277 [==============================] - 0s - loss: 2.0995 - mean_squared_error: 2.0995 - val_loss: 95.2205 - val_mean_squared_error: 95.2205\n",
      "Epoch 7/20\n",
      "3277/3277 [==============================] - 0s - loss: 1.2128 - mean_squared_error: 1.2128 - val_loss: 103.9974 - val_mean_squared_error: 103.9974\n",
      "Epoch 8/20\n",
      "3277/3277 [==============================] - 0s - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 108.1730 - val_mean_squared_error: 108.1730\n",
      "Epoch 9/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.9378 - mean_squared_error: 0.9378 - val_loss: 109.1274 - val_mean_squared_error: 109.1274\n",
      "Epoch 10/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.9376 - mean_squared_error: 0.9376 - val_loss: 110.3124 - val_mean_squared_error: 110.3124\n",
      "Epoch 11/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.9464 - mean_squared_error: 0.9464 - val_loss: 109.7063 - val_mean_squared_error: 109.7063\n",
      "Epoch 12/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.8659 - mean_squared_error: 0.8659 - val_loss: 110.5494 - val_mean_squared_error: 110.5494\n",
      "Epoch 13/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.8234 - mean_squared_error: 0.8234 - val_loss: 109.0008 - val_mean_squared_error: 109.0008\n",
      "Epoch 14/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.8709 - mean_squared_error: 0.8709 - val_loss: 110.0110 - val_mean_squared_error: 110.0110\n",
      "Epoch 15/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.8008 - mean_squared_error: 0.8008 - val_loss: 110.3517 - val_mean_squared_error: 110.3517\n",
      "Epoch 16/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.8129 - mean_squared_error: 0.8129 - val_loss: 109.8117 - val_mean_squared_error: 109.8117\n",
      "Epoch 17/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.8179 - mean_squared_error: 0.8179 - val_loss: 110.8022 - val_mean_squared_error: 110.8022\n",
      "Epoch 18/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.7665 - mean_squared_error: 0.7665 - val_loss: 111.1953 - val_mean_squared_error: 111.1953\n",
      "Epoch 19/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.7553 - mean_squared_error: 0.7553 - val_loss: 111.5753 - val_mean_squared_error: 111.5753\n",
      "Epoch 20/20\n",
      "3277/3277 [==============================] - 0s - loss: 0.7445 - mean_squared_error: 0.7445 - val_loss: 110.6870 - val_mean_squared_error: 110.6870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ba3740e6ba8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features, train_labels, epochs=20, batch_size=32,shuffle=True,\n",
    "                validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_predict = []\n",
    "test_data_sdss_id=[]\n",
    "for i in range(len(batch_data_test_object)):\n",
    "\n",
    "    test_predict.append(model.predict(test_features[i].reshape(1,-1))[0][0])\n",
    "    test_data_sdss_id.append(batch_data_test_object[i].SDSS_ID)\n",
    "    \n",
    "print(len(test_predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"pssid\":test_data_sdss_id,\"mass\":test_predict})\n",
    "df\n",
    "df[[\"pssid\",\"mass\"]].to_csv(os.path.join(output_path,\"sub_\"+version+\"_\"+str(image_size)+\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
