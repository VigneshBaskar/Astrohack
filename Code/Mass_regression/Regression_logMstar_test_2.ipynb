{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../Pre_processing\")\n",
    "\n",
    "from Data_Preparation_Library import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "image_size=128\n",
    "maxBatchId = 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selectedBatches=[\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"]\n",
    "selectedBatches=[str(i) for i in range(maxBatchId)]\n",
    "\n",
    "extra_folder=str(image_size)+\"_\"+str(image_size)\n",
    "batch_data_object = []\n",
    "for i in selectedBatches:\n",
    "    with open(os.path.join(temp_path,extra_folder,'full_data_object_' + i + '.p'), 'rb') as handle:\n",
    "        batch_data_object+=pickle.load(handle)\n",
    "\n",
    "data_train = batch_data_object[0:int(len(batch_data_object)*2/3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data_test_object = []\n",
    "for i in range(8):\n",
    "    with open(os.path.join(temp_path,extra_folder,'test_data_object_' + str(i) + '.p'), 'rb') as handle:\n",
    "        batch_data_test_object+=pickle.load(handle)\n",
    "        \n",
    "data_test  = batch_data_test_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flux(d):\n",
    "  # TODO: get flux_i from model\n",
    "    #if d.i_image != None:\n",
    "    #    flux_i = sum(sum(d.i_image))\n",
    "    #else:\n",
    "    #    flux_i = 0\n",
    "    flux_g = sum(sum(d.g_image_resized))\n",
    "    #return flux_i+flux_g, flux_g-flux_i\n",
    "    return flux_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8447, (128, 128, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_data_test_object),batch_data_test_object[0].g_image_resized_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "version=\"v1\"\n",
    "res_folder=\"results_\"+str(image_size)+\"_on_\"+str(image_size)\n",
    "with open(os.path.join(os.path.join(output_path,res_folder),\"encoder_results_train_\"+version+\"__all\"),'rb') as handle:\n",
    "    encoded_imgs1=pickle.load(handle)\n",
    "    encoded_imgs_reshaped_train = [e.reshape(-1,1) for e in encoded_imgs1]\n",
    "\n",
    "model_encoder=load_model(os.path.join(os.path.join(output_path,res_folder),\"encoder_model_\"+version+\"__all\"))\n",
    "\n",
    "def get_g_Data_for_Autoencoder(data_object):\n",
    "    X_list=[(data_object[index].g_image_resized_reshaped)/data_object[index].g_image_resized_reshaped.max() for index in range(len(data_object))]\n",
    "    return np.asarray(X_list),np.asarray(X_list)\n",
    "\n",
    "X_test_auto_encoder,y_test_auto_encoder=get_g_Data_for_Autoencoder(batch_data_test_object)\n",
    "\n",
    "encoded_imgs2=model_encoder.predict(X_test_auto_encoder)\n",
    "encoded_imgs_reshaped_test = [e.reshape(-1,1) for e in encoded_imgs2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import label\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "from sklearn.decomposition import PCA\n",
    "def clean_stars(img):\n",
    "    mask = img >= np.percentile(img, 96)\n",
    "    lbl = label(mask, background=0)\n",
    "    center_label = lbl[int(image_size/2),int(image_size/2)]\n",
    "    galaxy = lbl == center_label\n",
    "    stars = (lbl != center_label) & (lbl != 0) \n",
    "    image_clean = img / (1+ 5* stars + distance_transform_edt(~galaxy))\n",
    "    return mask, galaxy, lbl, image_clean\n",
    "def get_features(img):\n",
    "    f = {}\n",
    "    __, galaxy, lbl, image_clean = clean_stars(img)\n",
    "    f['flux'] = img[galaxy].sum()\n",
    "    gxy = np.array(np.nonzero(galaxy)).T\n",
    "    gxy_centered = gxy - gxy.mean(axis=0, keepdims=True)\n",
    "    pca = PCA().fit(gxy_centered)\n",
    "    f['pca_eigen1'] = pca.explained_variance_[0]\n",
    "    try:\n",
    "        f['pca_eigen2'] = pca.explained_variance_[1]\n",
    "    except:\n",
    "        f['pca_eigen2'] = 5.0\n",
    "    f['ellipticity'] = f['pca_eigen1'] / f['pca_eigen2']\n",
    "    f['minx'] = gxy[:,0].min()\n",
    "    f['maxx'] = gxy[:,0].max()\n",
    "    f['miny'] = gxy[:,1].min()\n",
    "    f['maxy'] = gxy[:,1].max()\n",
    "    distances = np.linalg.norm(gxy_centered, axis=1)\n",
    "    f['max_dist'] = distances.max()\n",
    "    f['median_dist'] = np.median(distances)\n",
    "    f['2d_first_moment'] = (distances * distances * img[np.nonzero(galaxy)]).sum()\n",
    "    f['max_lum'] = img[galaxy].max()\n",
    "    f['center_lum'] = img[int(image_size/2),int(image_size/2)]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15769, 12) (15769,)\n"
     ]
    }
   ],
   "source": [
    "# accessible in the class: size of image, maximum of image, normalization factor of g + fitted value of normalization factor for i\n",
    "# define a function to get the training and testing dataset\n",
    "\n",
    "features_no_star = ['flux','pca_eigen1','pca_eigen2', 'ellipticity', 'max_dist', 'median_dist', '2d_first_moment',\n",
    "                   'max_lum', 'center_lum']\n",
    "\n",
    "def obtain_train_data():\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    for i in range(len(data_train)):\n",
    "        if data_train[i].logMstar!=-99:\n",
    "            e = [] #encoded_imgs_reshaped_train[i]\n",
    "            d = data_train[i]\n",
    "            #f1,f2 = flux(d)\n",
    "            f = flux(d)\n",
    "            #size_1=d.g_image.shape[0]\n",
    "            #size_2=d.g_image.shape[1]\n",
    "            img_max=d.g_image_resized.max()\n",
    "            dist=d.Distance\n",
    "            no_star_f = get_features(d.g_image_resized)\n",
    "            train_features.append(np.append(e,\n",
    "                np.array([f,\n",
    "                #size_1,size_2,\n",
    "                img_max,dist] + [no_star_f[feat_name] for feat_name in features_no_star])))\n",
    "            #train_features.append(np.append(e,np.array([f])))\n",
    "            train_labels.append(d.logMstar)\n",
    "    return train_features, train_labels\n",
    "\n",
    "train_features = np.asarray(obtain_train_data()[0])\n",
    "train_labels = np.asarray(obtain_train_data()[1])\n",
    "\n",
    "print(train_features.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8447, 12) (8447,)\n"
     ]
    }
   ],
   "source": [
    "def obtain_test_data():\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    for j in range(len(data_test)):\n",
    "        if data_test[j].logMstar!=-99:\n",
    "            e = [] # encoded_imgs_reshaped_test[j]\n",
    "            d = data_test[j]\n",
    "\n",
    "            #f1,f2 = flux(d)\n",
    "            f = flux(d)\n",
    "            #size_1=d.g_image.shape[0]\n",
    "            #size_2=d.g_image.shape[1]\n",
    "            img_max=d.g_image_resized.max()\n",
    "            dist=d.Distance\n",
    "            no_star_f = get_features(d.g_image_resized)\n",
    "            test_features.append(np.append(e,\n",
    "                np.array([f,\n",
    "                                                       #size_1,size_2,\n",
    "                   img_max,dist] + [no_star_f[feat_name] for feat_name in features_no_star])))\n",
    "            test_labels.append(d.logMstar)\n",
    "\n",
    "    return test_features, test_labels\n",
    "\n",
    "test_features = np.asarray(obtain_test_data()[0])\n",
    "test_labels = np.asarray(obtain_test_data()[1])\n",
    "\n",
    "print(test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = np.nan_to_num(train_features)\n",
    "test_features = np.nan_to_num(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## transform features\n",
    "# # train_features = StandardScaler().fit_transform(train_features)\n",
    "# # test_features = StandardScaler().transform(test_features)\n",
    "# # #features = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "train_features = X_scaler.fit_transform(train_features)\n",
    "test_features = X_scaler.transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=np.exp(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_features=train_features/(train_features.max(0).T)\n",
    "# test_features=test_features/(train_features.max(0).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=train_features.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1,activation=\"relu\"))\n",
    "model.compile(optimizer='Adadelta',\n",
    "             loss='mse',\n",
    "             metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15769 samples, validate on 8447 samples\n",
      "Epoch 1/50\n",
      "15769/15769 [==============================] - 2s - loss: 805194261.3381 - mean_squared_error: 805194261.3381 - val_loss: 2755064113.7332 - val_mean_squared_error: 2755064113.7332\n",
      "Epoch 2/50\n",
      "15769/15769 [==============================] - 1s - loss: 238296319.9980 - mean_squared_error: 238296319.9980 - val_loss: 2402042267.9275 - val_mean_squared_error: 2402042267.9275\n",
      "Epoch 3/50\n",
      "15769/15769 [==============================] - 1s - loss: 239258438.0514 - mean_squared_error: 239258438.0514 - val_loss: 2444561475.0382 - val_mean_squared_error: 2444561475.0382\n",
      "Epoch 4/50\n",
      "15769/15769 [==============================] - 1s - loss: 232530774.7900 - mean_squared_error: 232530774.7900 - val_loss: 2471426872.3400 - val_mean_squared_error: 2471426872.3400\n",
      "Epoch 5/50\n",
      "15769/15769 [==============================] - 1s - loss: 238706461.0961 - mean_squared_error: 238706461.0961 - val_loss: 2465733629.6967 - val_mean_squared_error: 2465733629.6967\n",
      "Epoch 6/50\n",
      "15769/15769 [==============================] - 1s - loss: 234118998.0666 - mean_squared_error: 234118998.0666 - val_loss: 2476217344.1212 - val_mean_squared_error: 2476217344.1212\n",
      "Epoch 7/50\n",
      "15769/15769 [==============================] - 1s - loss: 235356154.6366 - mean_squared_error: 235356154.6366 - val_loss: 2457827065.9084 - val_mean_squared_error: 2457827065.9084\n",
      "Epoch 8/50\n",
      "15769/15769 [==============================] - 1s - loss: 236859417.3135 - mean_squared_error: 236859417.3135 - val_loss: 2472986950.9478 - val_mean_squared_error: 2472986950.9478\n",
      "Epoch 9/50\n",
      "15769/15769 [==============================] - 1s - loss: 234156667.3712 - mean_squared_error: 234156667.3712 - val_loss: 2466842542.2934 - val_mean_squared_error: 2466842542.2934\n",
      "Epoch 10/50\n",
      "15769/15769 [==============================] - 1s - loss: 237755578.4519 - mean_squared_error: 237755578.4519 - val_loss: 2465568480.6933 - val_mean_squared_error: 2465568480.6933\n",
      "Epoch 11/50\n",
      "15769/15769 [==============================] - 1s - loss: 234070354.0252 - mean_squared_error: 234070354.0252 - val_loss: 2470207333.6787 - val_mean_squared_error: 2470207333.6787\n",
      "Epoch 12/50\n",
      "15769/15769 [==============================] - 1s - loss: 237571656.5839 - mean_squared_error: 237571656.5839 - val_loss: 2451335451.9124 - val_mean_squared_error: 2451335451.9124\n",
      "Epoch 13/50\n",
      "15769/15769 [==============================] - 1s - loss: 233975289.1511 - mean_squared_error: 233975289.1511 - val_loss: 2463627165.9278 - val_mean_squared_error: 2463627165.9278\n",
      "Epoch 14/50\n",
      "15769/15769 [==============================] - 1s - loss: 230714447.6459 - mean_squared_error: 230714447.6459 - val_loss: 2468965905.1535 - val_mean_squared_error: 2468965905.1535\n",
      "Epoch 15/50\n",
      "15769/15769 [==============================] - 1s - loss: 233164828.6801 - mean_squared_error: 233164828.6801 - val_loss: 2462006965.2942 - val_mean_squared_error: 2462006965.2942\n",
      "Epoch 16/50\n",
      "15769/15769 [==============================] - 1s - loss: 231610011.4950 - mean_squared_error: 231610011.4950 - val_loss: 2459513683.4947 - val_mean_squared_error: 2459513683.4947\n",
      "Epoch 17/50\n",
      "15769/15769 [==============================] - 1s - loss: 232095567.4318 - mean_squared_error: 232095567.4318 - val_loss: 2470173840.0170 - val_mean_squared_error: 2470173840.0170\n",
      "Epoch 18/50\n",
      "15769/15769 [==============================] - 1s - loss: 232011071.9087 - mean_squared_error: 232011071.9087 - val_loss: 2473233233.5551 - val_mean_squared_error: 2473233233.5551\n",
      "Epoch 19/50\n",
      "15769/15769 [==============================] - 1s - loss: 231800148.8815 - mean_squared_error: 231800148.8815 - val_loss: 2474834525.1625 - val_mean_squared_error: 2474834525.1625\n",
      "Epoch 20/50\n",
      "15769/15769 [==============================] - 1s - loss: 232650464.8066 - mean_squared_error: 232650464.8066 - val_loss: 2478963758.0661 - val_mean_squared_error: 2478963758.0661\n",
      "Epoch 21/50\n",
      "15769/15769 [==============================] - 1s - loss: 233570083.4315 - mean_squared_error: 233570083.4315 - val_loss: 2472170414.1721 - val_mean_squared_error: 2472170414.1721\n",
      "Epoch 22/50\n",
      "15769/15769 [==============================] - 1s - loss: 229813377.0857 - mean_squared_error: 229813377.0857 - val_loss: 2468678748.4958 - val_mean_squared_error: 2468678748.4958\n",
      "Epoch 23/50\n",
      "15769/15769 [==============================] - 1s - loss: 229295159.0437 - mean_squared_error: 229295159.0437 - val_loss: 2470704297.1109 - val_mean_squared_error: 2470704297.1109\n",
      "Epoch 24/50\n",
      "15769/15769 [==============================] - 1s - loss: 228017434.6097 - mean_squared_error: 228017434.6097 - val_loss: 2472746313.3723 - val_mean_squared_error: 2472746313.3723\n",
      "Epoch 25/50\n",
      "15769/15769 [==============================] - 1s - loss: 225972759.5541 - mean_squared_error: 225972759.5541 - val_loss: 2473252497.1687 - val_mean_squared_error: 2473252497.1687\n",
      "Epoch 26/50\n",
      "15769/15769 [==============================] - 1s - loss: 228206835.6264 - mean_squared_error: 228206835.6264 - val_loss: 2481840138.9104 - val_mean_squared_error: 2481840138.9104\n",
      "Epoch 27/50\n",
      "15769/15769 [==============================] - 1s - loss: 230559959.2253 - mean_squared_error: 230559959.2253 - val_loss: 2475462073.9917 - val_mean_squared_error: 2475462073.9917\n",
      "Epoch 28/50\n",
      "15769/15769 [==============================] - 1s - loss: 228732147.5949 - mean_squared_error: 228732147.5949 - val_loss: 2477603183.5587 - val_mean_squared_error: 2477603183.5587\n",
      "Epoch 29/50\n",
      "15769/15769 [==============================] - 1s - loss: 224926825.8137 - mean_squared_error: 224926825.8137 - val_loss: 2472033011.9683 - val_mean_squared_error: 2472033011.9683\n",
      "Epoch 30/50\n",
      "15769/15769 [==============================] - 1s - loss: 227669563.6167 - mean_squared_error: 227669563.6167 - val_loss: 2469282849.3373 - val_mean_squared_error: 2469282849.3373\n",
      "Epoch 31/50\n",
      "15769/15769 [==============================] - 1s - loss: 226892880.7336 - mean_squared_error: 226892880.7336 - val_loss: 2464885894.1977 - val_mean_squared_error: 2464885894.1977\n",
      "Epoch 32/50\n",
      "15769/15769 [==============================] - 1s - loss: 223208627.1343 - mean_squared_error: 223208627.1343 - val_loss: 2473902898.0362 - val_mean_squared_error: 2473902898.0362\n",
      "Epoch 33/50\n",
      "15769/15769 [==============================] - 1s - loss: 223476053.0793 - mean_squared_error: 223476053.0793 - val_loss: 2472695741.0830 - val_mean_squared_error: 2472695741.0830\n",
      "Epoch 34/50\n",
      "15769/15769 [==============================] - 1s - loss: 220925847.2532 - mean_squared_error: 220925847.2532 - val_loss: 2474370126.7972 - val_mean_squared_error: 2474370126.7972\n",
      "Epoch 35/50\n",
      "15769/15769 [==============================] - 1s - loss: 223039466.6386 - mean_squared_error: 223039466.6386 - val_loss: 2467680503.6657 - val_mean_squared_error: 2467680503.6657\n",
      "Epoch 36/50\n",
      "15769/15769 [==============================] - 1s - loss: 221212041.0628 - mean_squared_error: 221212041.0628 - val_loss: 2467785689.2075 - val_mean_squared_error: 2467785689.2075\n",
      "Epoch 37/50\n",
      "15769/15769 [==============================] - 1s - loss: 220507203.8475 - mean_squared_error: 220507203.8475 - val_loss: 2486133619.9834 - val_mean_squared_error: 2486133619.9834\n",
      "Epoch 38/50\n",
      "15769/15769 [==============================] - 1s - loss: 220179616.8513 - mean_squared_error: 220179616.8513 - val_loss: 2482823206.7925 - val_mean_squared_error: 2482823206.7925\n",
      "Epoch 39/50\n",
      "15769/15769 [==============================] - 1s - loss: 223065616.6575 - mean_squared_error: 223065616.6575 - val_loss: 2469203535.7064 - val_mean_squared_error: 2469203535.7064\n",
      "Epoch 40/50\n",
      "15769/15769 [==============================] - 1s - loss: 220937926.8184 - mean_squared_error: 220937926.8184 - val_loss: 2484089512.8079 - val_mean_squared_error: 2484089512.8079\n",
      "Epoch 41/50\n",
      "15769/15769 [==============================] - 1s - loss: 221670756.3194 - mean_squared_error: 221670756.3194 - val_loss: 2473380733.5603 - val_mean_squared_error: 2473380733.5603\n",
      "Epoch 42/50\n",
      "15769/15769 [==============================] - 1s - loss: 218557090.0151 - mean_squared_error: 218557090.0151 - val_loss: 2473510028.7439 - val_mean_squared_error: 2473510028.7439\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15769/15769 [==============================] - 1s - loss: 218017847.4303 - mean_squared_error: 218017847.4303 - val_loss: 2468527264.6251 - val_mean_squared_error: 2468527264.6251\n",
      "Epoch 44/50\n",
      "15769/15769 [==============================] - 1s - loss: 220201640.2937 - mean_squared_error: 220201640.2937 - val_loss: 2486914516.7828 - val_mean_squared_error: 2486914516.7828\n",
      "Epoch 45/50\n",
      "15769/15769 [==============================] - 1s - loss: 216533214.1716 - mean_squared_error: 216533214.1716 - val_loss: 2478502581.9003 - val_mean_squared_error: 2478502581.9003\n",
      "Epoch 46/50\n",
      "15769/15769 [==============================] - 1s - loss: 217858005.6957 - mean_squared_error: 217858005.6957 - val_loss: 2470076621.2364 - val_mean_squared_error: 2470076621.2364\n",
      "Epoch 47/50\n",
      "15769/15769 [==============================] - 1s - loss: 218079911.9488 - mean_squared_error: 218079911.9488 - val_loss: 2471030465.9017 - val_mean_squared_error: 2471030465.9017\n",
      "Epoch 48/50\n",
      "15769/15769 [==============================] - 1s - loss: 216202060.5614 - mean_squared_error: 216202060.5614 - val_loss: 2467226030.8389 - val_mean_squared_error: 2467226030.8389\n",
      "Epoch 49/50\n",
      "15769/15769 [==============================] - 1s - loss: 216695962.7903 - mean_squared_error: 216695962.7903 - val_loss: 2475216340.2979 - val_mean_squared_error: 2475216340.2979\n",
      "Epoch 50/50\n",
      "15769/15769 [==============================] - 1s - loss: 216510720.1309 - mean_squared_error: 216510720.1309 - val_loss: 2471815970.3980 - val_mean_squared_error: 2471815970.3980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b9674c77208>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features, train_labels, epochs=50, batch_size=32,shuffle=True,\n",
    "                validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_predict = []\n",
    "test_data_sdss_id=[]\n",
    "for i in range(len(batch_data_test_object)):\n",
    "\n",
    "    test_predict.append(model.predict(test_features[i].reshape(1,-1))[0][0])\n",
    "    test_data_sdss_id.append(batch_data_test_object[i].SDSS_ID)\n",
    "    \n",
    "print(len(test_predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"pssid\":test_data_sdss_id,\"mass\":np.log(test_predict)})\n",
    "df\n",
    "df[[\"pssid\",\"mass\"]].to_csv(os.path.join(output_path,\"subMath_fix_\"+version+\"_\"+str(image_size)+\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass</th>\n",
       "      <th>pssid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.145727</td>\n",
       "      <td>1237662302977851635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.696742</td>\n",
       "      <td>1237652899687104664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.168529</td>\n",
       "      <td>1237649954407907520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.208810</td>\n",
       "      <td>1237661055281856813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.684472</td>\n",
       "      <td>1237651272957689913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mass                pssid\n",
       "0  10.145727  1237662302977851635\n",
       "1   9.696742  1237652899687104664\n",
       "2   9.168529  1237649954407907520\n",
       "3  10.208810  1237661055281856813\n",
       "4  11.684472  1237651272957689913"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
