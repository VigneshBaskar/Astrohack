{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../Pre_processing\")\n",
    "\n",
    "from Data_Preparation_Library import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_128  32_32\r\n"
     ]
    }
   ],
   "source": [
    "!ls /scratch/leuven/319/vsc31950/output_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selectedBatches=[\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"]\n",
    "maxBatchId = 1\n",
    "selectedBatches=[str(i) for i in range(maxBatchId)]\n",
    "\n",
    "finput = str(image_size)+\"_\"+str(image_size)\n",
    "\n",
    "batch_data_object = []\n",
    "for i in selectedBatches:\n",
    "    with open(os.path.join(temp_path,finput,'full_data_object_' + i + '.p'), 'rb') as handle:\n",
    "        batch_data_object+=pickle.load(handle)\n",
    "\n",
    "data_train = batch_data_object[0:int(len(batch_data_object)*2/3)]\n",
    "data_test  = batch_data_object[int(len(batch_data_object)*2/3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flux(d):\n",
    "  # TODO: get flux_i from model\n",
    "    if d.i_image != None:\n",
    "        flux_i = sum(sum(d.i_image))\n",
    "    else:\n",
    "        flux_i = 0\n",
    "    flux_g = sum(sum(d.g_image))\n",
    "    #return flux_i+flux_g, flux_g-flux_i\n",
    "    return flux_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresult = \"results_\"+str(image_size)+\"_on_\"+str(image_size)\n",
    "postfix = \"_all\"\n",
    "with open(os.path.join(output_path,fresult,\"encoder_results_train_v2_\" + postfix),'rb') as handle:\n",
    "    encoded_imgs1=pickle.load(handle)\n",
    "    encoded_imgs_reshaped_train = [e.reshape(-1,1) for e in encoded_imgs1]\n",
    "\n",
    "with open(os.path.join(output_path,fresult,\"encoder_results_test_v2_\"  + postfix),'rb') as handle:\n",
    "    encoded_imgs2=pickle.load(handle)\n",
    "    encoded_imgs_reshaped_test = [e.reshape(-1,1) for e in encoded_imgs2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def pca_feature(d):\n",
    "    \n",
    "#  return [v1_i,v2_i,v1_i/v2_i,v2_i/v1_i, v1_g,v2_g,v1_g/v2_g,v2_g/v1_g]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import label\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "from sklearn.decomposition import PCA\n",
    "def clean_stars(img):\n",
    "    mask = img >= np.percentile(img, 96)\n",
    "    lbl = label(mask, background=0)\n",
    "    center_label = lbl[image_size//2,image_size // 2]\n",
    "    galaxy = lbl == center_label\n",
    "    stars = (lbl != center_label) & (lbl != 0) \n",
    "    image_clean = img / (1+ 5* stars + distance_transform_edt(~galaxy))\n",
    "    return mask, galaxy, lbl, image_clean\n",
    "def get_features(img):\n",
    "    f = {}\n",
    "    __, galaxy, lbl, image_clean = clean_stars(img)\n",
    "    f['flux'] = img[galaxy].sum()\n",
    "    gxy = np.array(np.nonzero(galaxy)).T\n",
    "    gxy_centered = gxy - gxy.mean(axis=0, keepdims=True)\n",
    "    pca = PCA().fit(gxy_centered)\n",
    "    f['pca_eigen1'] = pca.explained_variance_[0]\n",
    "    f['pca_eigen2'] = pca.explained_variance_[1]\n",
    "    f['ellipticity'] = f['pca_eigen1'] / f['pca_eigen2']\n",
    "    f['minx'] = gxy[:,0].min()\n",
    "    f['maxx'] = gxy[:,0].max()\n",
    "    f['miny'] = gxy[:,1].min()\n",
    "    f['maxy'] = gxy[:,1].max()\n",
    "    distances = np.linalg.norm(gxy_centered, axis=1)\n",
    "    f['max_dist'] = distances.max()\n",
    "    f['median_dist'] = np.median(distances)\n",
    "    f['2d_first_moment'] = (distances * distances * img[np.nonzero(galaxy)]).sum()\n",
    "    f['max_intensity'] = img[galaxy].max()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(659, 268) (659,)\n",
      "[  1.90987945e-01   8.84844005e-01   0.00000000e+00   6.79106936e-02\n",
      "   1.94844604e-01   8.77789378e-01   0.00000000e+00   1.86601095e-02\n",
      "   1.84904635e-01   8.52527380e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.14054757e-01   7.65758157e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.94813389e-01   7.81931818e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.61049801e-01   8.55804801e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.59723842e-01   8.78644705e-01   0.00000000e+00   2.81448197e-02\n",
      "   2.21107483e-01   8.80904675e-01   0.00000000e+00   5.44141307e-02\n",
      "   1.86676353e-01   8.84442687e-01   0.00000000e+00   6.30877316e-02\n",
      "   1.85942948e-01   8.75049472e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.71866149e-01   8.39093864e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.96946794e-01   7.31972516e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.75833005e-01   7.51925766e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.89844328e-01   8.42040360e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.52513677e-01   8.74023795e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.23832905e-01   8.80926251e-01   0.00000000e+00   6.35322928e-02\n",
      "   1.98022068e-01   8.83877516e-01   0.00000000e+00   6.13956302e-02\n",
      "   1.87737197e-01   8.69948268e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.65066302e-01   8.14720631e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.04583490e-01   6.69561565e-01   0.00000000e+00   0.00000000e+00\n",
      "   4.59761590e-01   6.92984462e-01   0.00000000e+00   0.00000000e+00\n",
      "   4.21136171e-01   8.28365684e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.81810224e-01   8.71705890e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.24897236e-01   8.84044945e-01   0.00000000e+00   7.75870681e-02\n",
      "   1.83514625e-01   8.77851248e-01   0.00000000e+00   4.18230779e-02\n",
      "   1.85708463e-01   8.66362631e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.98500812e-01   7.90338159e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.13586754e-01   5.66146612e-01   0.00000000e+00   0.00000000e+00\n",
      "   6.85690522e-01   5.44925809e-01   0.00000000e+00   0.00000000e+00\n",
      "   4.62818295e-01   8.05730224e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.78595746e-01   8.69210184e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.29667127e-01   8.82281840e-01   0.00000000e+00   6.77436441e-02\n",
      "   1.89711124e-01   8.80113125e-01   0.00000000e+00   5.38422242e-02\n",
      "   1.80133820e-01   8.64200652e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.80956900e-01   7.90322423e-01   0.00000000e+00   0.00000000e+00\n",
      "   9.57132578e-02   4.22863215e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.92774320e-01   5.13487279e-01   0.00000000e+00   0.00000000e+00\n",
      "   4.07462388e-01   7.96521306e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.84988195e-01   8.69452119e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.31710374e-01   8.78099203e-01   0.00000000e+00   4.27280702e-02\n",
      "   1.84132338e-01   8.79678845e-01   0.00000000e+00   4.54000086e-02\n",
      "   1.86887532e-01   8.67562532e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.63719803e-01   8.05778801e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.54382163e-01   6.63930595e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.36828619e-01   6.52711511e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.62323314e-01   8.19549799e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.51019955e-01   8.66464019e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.34416574e-01   8.79914641e-01   0.00000000e+00   6.78070039e-02\n",
      "   1.87439263e-01   8.77916276e-01   0.00000000e+00   4.31713536e-02\n",
      "   2.12261438e-01   8.65570188e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.67828292e-01   8.27729464e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.65662044e-01   7.24592745e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.39691550e-01   7.33641028e-01   0.00000000e+00   0.00000000e+00\n",
      "   3.30296129e-01   8.43376160e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.40210265e-01   8.78219485e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.15391606e-01   8.84556651e-01   0.00000000e+00   6.21676892e-02\n",
      "   1.73096478e-01   8.65513325e-01   0.00000000e+00   3.43902707e-02\n",
      "   1.92672551e-01   8.54917169e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.73338145e-01   8.32103372e-01   0.00000000e+00   0.00000000e+00\n",
      "   1.82730436e-01   7.50135660e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.80797839e-01   7.64752209e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.45709330e-01   8.46100569e-01   0.00000000e+00   0.00000000e+00\n",
      "   2.25009590e-01   8.77377629e-01   0.00000000e+00   2.06077378e-02\n",
      "   2.06779212e-01   8.85953307e-01   0.00000000e+00   9.95814502e-02\n",
      "   2.35405851e+02   1.66000000e+02   1.66000000e+02   3.67713000e+00\n",
      "   5.17248936e+01   1.40155335e+03   1.39427881e+03   1.00521743e+00\n",
      "   9.00051456e+01   5.16596502e+01   1.23798852e+05   3.61226804e-02]\n"
     ]
    }
   ],
   "source": [
    "# accessible in the class: size of image, maximum of image, normalization factor of g + fitted value of normalization factor for i\n",
    "# define a function to get the training and testing dataset\n",
    "no_star_features = ['flux', 'pca_eigen1', 'pca_eigen2', 'ellipticity', 'max_dist',\n",
    "                    'median_dist', '2d_first_moment', 'max_intensity']\n",
    "def obtain_train_data():\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    for i in range(len(data_train)):\n",
    "        if data_train[i].logMstar!=-99:\n",
    "            e = encoded_imgs_reshaped_train[i]\n",
    "            \n",
    "            d = data_train[i]\n",
    "            #f1,f2 = flux(d)\n",
    "            f = flux(d)\n",
    "            size_1=d.g_image.shape[0]\n",
    "            size_2=d.g_image.shape[1]\n",
    "            no_star_f = get_features(d.g_image_resized)\n",
    "            img_max=d.g_image.max()\n",
    "            train_features.append(np.append(e,np.array([f,size_1,size_2,img_max] + [no_star_f[feat_name] for feat_name in no_star_features])))\n",
    "            train_labels.append(d.logMstar)\n",
    "    return train_features, train_labels\n",
    "\n",
    "train_features = np.asarray(obtain_train_data()[0])\n",
    "train_labels = np.asarray(obtain_train_data()[1])\n",
    "\n",
    "print(train_features.shape, train_labels.shape)\n",
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325, 4100) (325,)\n"
     ]
    }
   ],
   "source": [
    "def obtain_test_data():\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    for j in range(len(data_test)):\n",
    "        if data_test[j].logMstar!=-99:\n",
    "            e = encoded_imgs_reshaped_test[j]\n",
    "            d = data_test[j]\n",
    "\n",
    "        #f1,f2 = flux(d)\n",
    "            f = flux(d)\n",
    "            size_1=d.g_image.shape[0]\n",
    "            size_2=d.g_image.shape[1]\n",
    "            img_max=d.g_image.max()\n",
    "            test_features.append(np.append(e,np.array([f,size_1,size_2,img_max])))\n",
    "            test_labels.append(d.logMstar)\n",
    "\n",
    "    return test_features, test_labels\n",
    "\n",
    "test_features = np.asarray(obtain_test_data()[0])\n",
    "test_labels = np.asarray(obtain_test_data()[1])\n",
    "\n",
    "print(test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.46823421 -0.5080603   0.02156935 ..., -0.08512842 -0.08512842\n",
      "  -0.31407447]\n",
      " [-0.37778186  0.01002213  0.36101057 ...,  1.97956789  1.97956789\n",
      "  -0.3734218 ]\n",
      " [-0.31466022 -0.04653447  0.02059651 ...,  1.35231838  1.35231838\n",
      "   1.8731427 ]\n",
      " [-0.34544549  0.51801735  0.59461292 ..., -0.45102397 -0.45102397\n",
      "  -0.4427597 ]\n",
      " [ 0.5452832   1.31677986  0.74994534 ..., -0.56863325 -0.56863325\n",
      "  -0.42390013]] [[ 0.15133955 -1.55998065 -0.05018915 ...,  0.01941317  0.01941317\n",
      "  -0.41906929]\n",
      " [-0.31818065  0.05129635  0.23653529 ...,  1.888094    1.888094\n",
      "  -0.11246488]\n",
      " [ 0.8351674   1.77883233  1.02854044 ..., -0.49022706 -0.49022706\n",
      "  -0.44070063]\n",
      " [-0.48722797 -0.36853051  0.03476775 ...,  0.01941317  0.01941317\n",
      "  -0.30787221]\n",
      " [-0.51023902 -1.52314199 -1.47030972 ..., -0.52943016 -0.52943016\n",
      "  -0.42655866]]\n"
     ]
    }
   ],
   "source": [
    "# ## transform features\n",
    "# train_features = StandardScaler().fit_transform(train_features)\n",
    "# test_features = StandardScaler().transform(test_features)\n",
    "# #features = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "train_features = X_scaler.fit_transform(train_features)\n",
    "test_features = X_scaler.transform(test_features)\n",
    "\n",
    "print(train_features[:5], test_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid', input_dim=train_features.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='Adadelta',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 659 samples, validate on 325 samples\n",
      "Epoch 1/100\n",
      "659/659 [==============================] - 0s - loss: 116.7597 - mean_squared_error: 116.7597 - val_loss: 111.0752 - val_mean_squared_error: 111.0752\n",
      "Epoch 2/100\n",
      "659/659 [==============================] - 0s - loss: 102.1920 - mean_squared_error: 102.1920 - val_loss: 98.6557 - val_mean_squared_error: 98.6557\n",
      "Epoch 3/100\n",
      "659/659 [==============================] - 0s - loss: 90.6146 - mean_squared_error: 90.6146 - val_loss: 87.7422 - val_mean_squared_error: 87.7422\n",
      "Epoch 4/100\n",
      "659/659 [==============================] - 0s - loss: 80.9058 - mean_squared_error: 80.9058 - val_loss: 78.5043 - val_mean_squared_error: 78.5043\n",
      "Epoch 5/100\n",
      "659/659 [==============================] - 0s - loss: 72.3109 - mean_squared_error: 72.3109 - val_loss: 70.6543 - val_mean_squared_error: 70.6543\n",
      "Epoch 6/100\n",
      "659/659 [==============================] - 0s - loss: 64.9769 - mean_squared_error: 64.9769 - val_loss: 64.0039 - val_mean_squared_error: 64.0039\n",
      "Epoch 7/100\n",
      "659/659 [==============================] - 0s - loss: 58.5468 - mean_squared_error: 58.5468 - val_loss: 58.3234 - val_mean_squared_error: 58.3234\n",
      "Epoch 8/100\n",
      "659/659 [==============================] - 0s - loss: 53.4841 - mean_squared_error: 53.4841 - val_loss: 53.4438 - val_mean_squared_error: 53.4438\n",
      "Epoch 9/100\n",
      "659/659 [==============================] - 0s - loss: 49.1341 - mean_squared_error: 49.1341 - val_loss: 49.0756 - val_mean_squared_error: 49.0756\n",
      "Epoch 10/100\n",
      "659/659 [==============================] - 0s - loss: 44.9667 - mean_squared_error: 44.9667 - val_loss: 45.1749 - val_mean_squared_error: 45.1749\n",
      "Epoch 11/100\n",
      "659/659 [==============================] - 0s - loss: 40.7470 - mean_squared_error: 40.7470 - val_loss: 41.5944 - val_mean_squared_error: 41.5944\n",
      "Epoch 12/100\n",
      "659/659 [==============================] - 0s - loss: 37.7694 - mean_squared_error: 37.7694 - val_loss: 38.4207 - val_mean_squared_error: 38.4207\n",
      "Epoch 13/100\n",
      "659/659 [==============================] - 0s - loss: 34.7035 - mean_squared_error: 34.7035 - val_loss: 35.4542 - val_mean_squared_error: 35.4542\n",
      "Epoch 14/100\n",
      "659/659 [==============================] - 0s - loss: 32.3508 - mean_squared_error: 32.3508 - val_loss: 32.6926 - val_mean_squared_error: 32.6926\n",
      "Epoch 15/100\n",
      "659/659 [==============================] - 0s - loss: 29.5101 - mean_squared_error: 29.5101 - val_loss: 30.0605 - val_mean_squared_error: 30.0605\n",
      "Epoch 16/100\n",
      "659/659 [==============================] - 0s - loss: 26.8802 - mean_squared_error: 26.8802 - val_loss: 27.6083 - val_mean_squared_error: 27.6083\n",
      "Epoch 17/100\n",
      "659/659 [==============================] - 0s - loss: 24.5064 - mean_squared_error: 24.5064 - val_loss: 25.3779 - val_mean_squared_error: 25.3779\n",
      "Epoch 18/100\n",
      "659/659 [==============================] - 0s - loss: 22.5231 - mean_squared_error: 22.5231 - val_loss: 23.2775 - val_mean_squared_error: 23.2775\n",
      "Epoch 19/100\n",
      "659/659 [==============================] - 0s - loss: 20.4096 - mean_squared_error: 20.4096 - val_loss: 21.3364 - val_mean_squared_error: 21.3364\n",
      "Epoch 20/100\n",
      "659/659 [==============================] - 0s - loss: 18.9405 - mean_squared_error: 18.9405 - val_loss: 19.5578 - val_mean_squared_error: 19.5578\n",
      "Epoch 21/100\n",
      "659/659 [==============================] - 0s - loss: 17.3245 - mean_squared_error: 17.3245 - val_loss: 17.8307 - val_mean_squared_error: 17.8307\n",
      "Epoch 22/100\n",
      "659/659 [==============================] - 0s - loss: 15.8874 - mean_squared_error: 15.8874 - val_loss: 15.9759 - val_mean_squared_error: 15.9759\n",
      "Epoch 23/100\n",
      "659/659 [==============================] - 0s - loss: 13.6843 - mean_squared_error: 13.6843 - val_loss: 14.2523 - val_mean_squared_error: 14.2523\n",
      "Epoch 24/100\n",
      "659/659 [==============================] - 0s - loss: 12.6965 - mean_squared_error: 12.6965 - val_loss: 12.6767 - val_mean_squared_error: 12.6767\n",
      "Epoch 25/100\n",
      "659/659 [==============================] - 0s - loss: 11.2967 - mean_squared_error: 11.2967 - val_loss: 11.2467 - val_mean_squared_error: 11.2467\n",
      "Epoch 26/100\n",
      "659/659 [==============================] - 0s - loss: 9.8712 - mean_squared_error: 9.8712 - val_loss: 9.9189 - val_mean_squared_error: 9.9189\n",
      "Epoch 27/100\n",
      "659/659 [==============================] - 0s - loss: 8.6008 - mean_squared_error: 8.6008 - val_loss: 8.3438 - val_mean_squared_error: 8.3438\n",
      "Epoch 28/100\n",
      "659/659 [==============================] - 0s - loss: 7.0137 - mean_squared_error: 7.0137 - val_loss: 7.0406 - val_mean_squared_error: 7.0406\n",
      "Epoch 29/100\n",
      "659/659 [==============================] - 0s - loss: 6.0001 - mean_squared_error: 6.0001 - val_loss: 5.9178 - val_mean_squared_error: 5.9178\n",
      "Epoch 30/100\n",
      "659/659 [==============================] - 0s - loss: 5.0651 - mean_squared_error: 5.0651 - val_loss: 4.9970 - val_mean_squared_error: 4.9970\n",
      "Epoch 31/100\n",
      "659/659 [==============================] - 0s - loss: 4.0968 - mean_squared_error: 4.0968 - val_loss: 4.2263 - val_mean_squared_error: 4.2263\n",
      "Epoch 32/100\n",
      "659/659 [==============================] - 0s - loss: 3.6465 - mean_squared_error: 3.6465 - val_loss: 3.5686 - val_mean_squared_error: 3.5686\n",
      "Epoch 33/100\n",
      "659/659 [==============================] - 0s - loss: 3.3687 - mean_squared_error: 3.3687 - val_loss: 2.9913 - val_mean_squared_error: 2.9913\n",
      "Epoch 34/100\n",
      "659/659 [==============================] - 0s - loss: 2.5886 - mean_squared_error: 2.5886 - val_loss: 2.5285 - val_mean_squared_error: 2.5285\n",
      "Epoch 35/100\n",
      "659/659 [==============================] - 0s - loss: 2.4260 - mean_squared_error: 2.4260 - val_loss: 2.1385 - val_mean_squared_error: 2.1385\n",
      "Epoch 36/100\n",
      "659/659 [==============================] - 0s - loss: 1.9168 - mean_squared_error: 1.9168 - val_loss: 1.8121 - val_mean_squared_error: 1.8121\n",
      "Epoch 37/100\n",
      "659/659 [==============================] - 0s - loss: 1.8582 - mean_squared_error: 1.8582 - val_loss: 1.5325 - val_mean_squared_error: 1.5325\n",
      "Epoch 38/100\n",
      "659/659 [==============================] - 0s - loss: 1.7980 - mean_squared_error: 1.7980 - val_loss: 1.3070 - val_mean_squared_error: 1.3070\n",
      "Epoch 39/100\n",
      "659/659 [==============================] - 0s - loss: 1.5253 - mean_squared_error: 1.5253 - val_loss: 1.1317 - val_mean_squared_error: 1.1317\n",
      "Epoch 40/100\n",
      "659/659 [==============================] - 0s - loss: 1.2612 - mean_squared_error: 1.2612 - val_loss: 0.9807 - val_mean_squared_error: 0.9807\n",
      "Epoch 41/100\n",
      "659/659 [==============================] - 0s - loss: 1.3089 - mean_squared_error: 1.3089 - val_loss: 0.8647 - val_mean_squared_error: 0.8647\n",
      "Epoch 42/100\n",
      "659/659 [==============================] - 0s - loss: 1.1672 - mean_squared_error: 1.1672 - val_loss: 0.7762 - val_mean_squared_error: 0.7762\n",
      "Epoch 43/100\n",
      "659/659 [==============================] - 0s - loss: 1.2862 - mean_squared_error: 1.2862 - val_loss: 0.6669 - val_mean_squared_error: 0.6669\n",
      "Epoch 44/100\n",
      "659/659 [==============================] - 0s - loss: 1.0828 - mean_squared_error: 1.0828 - val_loss: 0.6068 - val_mean_squared_error: 0.6068\n",
      "Epoch 45/100\n",
      "659/659 [==============================] - 0s - loss: 1.0041 - mean_squared_error: 1.0041 - val_loss: 0.5779 - val_mean_squared_error: 0.5779\n",
      "Epoch 46/100\n",
      "659/659 [==============================] - 0s - loss: 1.0808 - mean_squared_error: 1.0808 - val_loss: 0.5638 - val_mean_squared_error: 0.5638\n",
      "Epoch 47/100\n",
      "659/659 [==============================] - 0s - loss: 1.0657 - mean_squared_error: 1.0657 - val_loss: 0.5203 - val_mean_squared_error: 0.5203\n",
      "Epoch 48/100\n",
      "659/659 [==============================] - 0s - loss: 0.9572 - mean_squared_error: 0.9572 - val_loss: 0.5087 - val_mean_squared_error: 0.5087\n",
      "Epoch 49/100\n",
      "659/659 [==============================] - 0s - loss: 0.9778 - mean_squared_error: 0.9778 - val_loss: 0.5117 - val_mean_squared_error: 0.5117\n",
      "Epoch 50/100\n",
      "659/659 [==============================] - 0s - loss: 0.9904 - mean_squared_error: 0.9904 - val_loss: 0.4782 - val_mean_squared_error: 0.4782\n",
      "Epoch 51/100\n",
      "659/659 [==============================] - 0s - loss: 1.0013 - mean_squared_error: 1.0013 - val_loss: 0.4611 - val_mean_squared_error: 0.4611\n",
      "Epoch 52/100\n",
      "659/659 [==============================] - 0s - loss: 0.8924 - mean_squared_error: 0.8924 - val_loss: 0.4753 - val_mean_squared_error: 0.4753\n",
      "Epoch 53/100\n",
      "659/659 [==============================] - 0s - loss: 1.0029 - mean_squared_error: 1.0029 - val_loss: 0.5099 - val_mean_squared_error: 0.5099\n",
      "Epoch 54/100\n",
      "659/659 [==============================] - 0s - loss: 0.9554 - mean_squared_error: 0.9554 - val_loss: 0.4669 - val_mean_squared_error: 0.4669\n",
      "Epoch 55/100\n",
      "659/659 [==============================] - 0s - loss: 1.0432 - mean_squared_error: 1.0432 - val_loss: 0.4693 - val_mean_squared_error: 0.4693\n",
      "Epoch 56/100\n",
      "659/659 [==============================] - 0s - loss: 0.9329 - mean_squared_error: 0.9329 - val_loss: 0.4611 - val_mean_squared_error: 0.4611\n",
      "Epoch 57/100\n",
      "659/659 [==============================] - 0s - loss: 0.9438 - mean_squared_error: 0.9438 - val_loss: 0.4631 - val_mean_squared_error: 0.4631\n",
      "Epoch 58/100\n",
      "659/659 [==============================] - 0s - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 0.4558 - val_mean_squared_error: 0.4558\n",
      "Epoch 59/100\n",
      "659/659 [==============================] - 0s - loss: 1.0141 - mean_squared_error: 1.0141 - val_loss: 0.4562 - val_mean_squared_error: 0.4562\n",
      "Epoch 60/100\n",
      "659/659 [==============================] - 0s - loss: 0.9684 - mean_squared_error: 0.9684 - val_loss: 0.4533 - val_mean_squared_error: 0.4533\n",
      "Epoch 61/100\n",
      "659/659 [==============================] - 0s - loss: 0.8800 - mean_squared_error: 0.8800 - val_loss: 0.4780 - val_mean_squared_error: 0.4780\n",
      "Epoch 62/100\n",
      "659/659 [==============================] - 0s - loss: 0.9853 - mean_squared_error: 0.9853 - val_loss: 0.4637 - val_mean_squared_error: 0.4637\n",
      "Epoch 63/100\n",
      "659/659 [==============================] - 0s - loss: 0.9893 - mean_squared_error: 0.9893 - val_loss: 0.4323 - val_mean_squared_error: 0.4323\n",
      "Epoch 64/100\n",
      "659/659 [==============================] - 0s - loss: 0.8696 - mean_squared_error: 0.8696 - val_loss: 0.4621 - val_mean_squared_error: 0.4621\n",
      "Epoch 65/100\n",
      "659/659 [==============================] - 0s - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 0.4512 - val_mean_squared_error: 0.4512\n",
      "Epoch 66/100\n",
      "659/659 [==============================] - 0s - loss: 1.0028 - mean_squared_error: 1.0028 - val_loss: 0.4255 - val_mean_squared_error: 0.4255\n",
      "Epoch 67/100\n",
      "659/659 [==============================] - 0s - loss: 0.9203 - mean_squared_error: 0.9203 - val_loss: 0.4591 - val_mean_squared_error: 0.4591\n",
      "Epoch 68/100\n",
      "659/659 [==============================] - 0s - loss: 0.8722 - mean_squared_error: 0.8722 - val_loss: 0.4615 - val_mean_squared_error: 0.4615\n",
      "Epoch 69/100\n",
      "659/659 [==============================] - 0s - loss: 1.0077 - mean_squared_error: 1.0077 - val_loss: 0.4447 - val_mean_squared_error: 0.4447\n",
      "Epoch 70/100\n",
      "659/659 [==============================] - 0s - loss: 0.9313 - mean_squared_error: 0.9313 - val_loss: 0.4732 - val_mean_squared_error: 0.4732\n",
      "Epoch 71/100\n",
      "659/659 [==============================] - 0s - loss: 0.9785 - mean_squared_error: 0.9785 - val_loss: 0.4650 - val_mean_squared_error: 0.4650\n",
      "Epoch 72/100\n",
      "659/659 [==============================] - 0s - loss: 0.8904 - mean_squared_error: 0.8904 - val_loss: 0.4610 - val_mean_squared_error: 0.4610\n",
      "Epoch 73/100\n",
      "659/659 [==============================] - 0s - loss: 0.8750 - mean_squared_error: 0.8750 - val_loss: 0.4575 - val_mean_squared_error: 0.4575\n",
      "Epoch 74/100\n",
      "659/659 [==============================] - 0s - loss: 0.9227 - mean_squared_error: 0.9227 - val_loss: 0.4579 - val_mean_squared_error: 0.4579\n",
      "Epoch 75/100\n",
      "659/659 [==============================] - 0s - loss: 0.9223 - mean_squared_error: 0.9223 - val_loss: 0.4363 - val_mean_squared_error: 0.4363\n",
      "Epoch 76/100\n",
      "659/659 [==============================] - 0s - loss: 0.9823 - mean_squared_error: 0.9823 - val_loss: 0.4458 - val_mean_squared_error: 0.4458\n",
      "Epoch 77/100\n",
      "659/659 [==============================] - 0s - loss: 0.8639 - mean_squared_error: 0.8639 - val_loss: 0.4573 - val_mean_squared_error: 0.4573\n",
      "Epoch 78/100\n",
      "659/659 [==============================] - 0s - loss: 0.8684 - mean_squared_error: 0.8684 - val_loss: 0.4432 - val_mean_squared_error: 0.4432\n",
      "Epoch 79/100\n",
      "659/659 [==============================] - 0s - loss: 0.9309 - mean_squared_error: 0.9309 - val_loss: 0.4420 - val_mean_squared_error: 0.4420\n",
      "Epoch 80/100\n",
      "659/659 [==============================] - 0s - loss: 0.9736 - mean_squared_error: 0.9736 - val_loss: 0.4373 - val_mean_squared_error: 0.4373\n",
      "Epoch 81/100\n",
      "659/659 [==============================] - 0s - loss: 0.9062 - mean_squared_error: 0.9062 - val_loss: 0.4571 - val_mean_squared_error: 0.4571\n",
      "Epoch 82/100\n",
      "659/659 [==============================] - 0s - loss: 0.8166 - mean_squared_error: 0.8166 - val_loss: 0.4570 - val_mean_squared_error: 0.4570\n",
      "Epoch 83/100\n",
      "659/659 [==============================] - 0s - loss: 0.8669 - mean_squared_error: 0.8669 - val_loss: 0.4620 - val_mean_squared_error: 0.4620\n",
      "Epoch 84/100\n",
      "659/659 [==============================] - 0s - loss: 0.8775 - mean_squared_error: 0.8775 - val_loss: 0.4633 - val_mean_squared_error: 0.4633\n",
      "Epoch 85/100\n",
      "659/659 [==============================] - 0s - loss: 0.8552 - mean_squared_error: 0.8552 - val_loss: 0.4389 - val_mean_squared_error: 0.4389\n",
      "Epoch 86/100\n",
      "659/659 [==============================] - 0s - loss: 0.8062 - mean_squared_error: 0.8062 - val_loss: 0.4585 - val_mean_squared_error: 0.4585\n",
      "Epoch 87/100\n",
      "659/659 [==============================] - 0s - loss: 0.9274 - mean_squared_error: 0.9274 - val_loss: 0.4691 - val_mean_squared_error: 0.4691\n",
      "Epoch 88/100\n",
      "659/659 [==============================] - 0s - loss: 0.8460 - mean_squared_error: 0.8460 - val_loss: 0.4374 - val_mean_squared_error: 0.4374\n",
      "Epoch 89/100\n",
      "659/659 [==============================] - 0s - loss: 0.8519 - mean_squared_error: 0.8519 - val_loss: 0.4522 - val_mean_squared_error: 0.4522\n",
      "Epoch 90/100\n",
      "659/659 [==============================] - 0s - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 0.4371 - val_mean_squared_error: 0.4371\n",
      "Epoch 91/100\n",
      "659/659 [==============================] - 0s - loss: 0.8235 - mean_squared_error: 0.8235 - val_loss: 0.4308 - val_mean_squared_error: 0.4308\n",
      "Epoch 92/100\n",
      "659/659 [==============================] - 0s - loss: 0.8634 - mean_squared_error: 0.8634 - val_loss: 0.4173 - val_mean_squared_error: 0.4173\n",
      "Epoch 93/100\n",
      "659/659 [==============================] - 0s - loss: 0.8009 - mean_squared_error: 0.8009 - val_loss: 0.4529 - val_mean_squared_error: 0.4529\n",
      "Epoch 94/100\n",
      "659/659 [==============================] - 0s - loss: 0.8573 - mean_squared_error: 0.8573 - val_loss: 0.4291 - val_mean_squared_error: 0.4291\n",
      "Epoch 95/100\n",
      "659/659 [==============================] - 0s - loss: 0.7524 - mean_squared_error: 0.7524 - val_loss: 0.4428 - val_mean_squared_error: 0.4428\n",
      "Epoch 96/100\n",
      "659/659 [==============================] - 0s - loss: 0.8391 - mean_squared_error: 0.8391 - val_loss: 0.4474 - val_mean_squared_error: 0.4474\n",
      "Epoch 97/100\n",
      "659/659 [==============================] - 0s - loss: 0.8520 - mean_squared_error: 0.8520 - val_loss: 0.4387 - val_mean_squared_error: 0.4387\n",
      "Epoch 98/100\n",
      "659/659 [==============================] - 0s - loss: 0.7995 - mean_squared_error: 0.7995 - val_loss: 0.4424 - val_mean_squared_error: 0.4424\n",
      "Epoch 99/100\n",
      "659/659 [==============================] - 0s - loss: 0.7770 - mean_squared_error: 0.7770 - val_loss: 0.4412 - val_mean_squared_error: 0.4412\n",
      "Epoch 100/100\n",
      "659/659 [==============================] - 0s - loss: 0.7808 - mean_squared_error: 0.7808 - val_loss: 0.4266 - val_mean_squared_error: 0.4266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b8d14d03b38>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features, train_labels, epochs=100, batch_size=32,shuffle=True,\n",
    "                validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, numpy.float64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_predict),type(test_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.471828023804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_predict = []\n",
    "test_labels_2=[]\n",
    "for i in range(100):\n",
    "    if data_test[i].logMstar!=-99:\n",
    "        test_predict.append(model.predict(test_features[i].reshape(1,-1))[0][0])\n",
    "        test_labels_2.append(test_labels[i])\n",
    "print(len(test_predict))\n",
    "print(np.sqrt(mean_squared_error(np.asarray(test_predict), np.asarray(test_labels_2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
