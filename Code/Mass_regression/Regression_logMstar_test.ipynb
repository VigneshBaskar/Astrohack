{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../Pre_processing\")\n",
    "\n",
    "from Data_Preparation_Library import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selectedBatches=[\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"]\n",
    "maxBatchId = 1\n",
    "selectedBatches=[str(i) for i in range(maxBatchId)]\n",
    "\n",
    "extra_folder=\"128_128\"\n",
    "batch_data_object = []\n",
    "for i in selectedBatches:\n",
    "    with open(os.path.join(temp_path,extra_folder,'full_data_object_' + i + '.p'), 'rb') as handle:\n",
    "        batch_data_object+=pickle.load(handle)\n",
    "\n",
    "data_train = batch_data_object[0:int(len(batch_data_object)*2/3)]\n",
    "#data_test  = batch_data_object[int(len(batch_data_object)*2/3):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extra_folder=\"128_128\"\n",
    "batch_data_test_object = []\n",
    "for i in range(8):\n",
    "    with open(os.path.join(temp_path,extra_folder,'test_data_object_' + str(i) + '.p'), 'rb') as handle:\n",
    "        batch_data_test_object+=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test  = batch_data_test_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flux(d):\n",
    "  # TODO: get flux_i from model\n",
    "    if d.i_image != None:\n",
    "        flux_i = sum(sum(d.i_image))#\n",
    "    else:\n",
    "        flux_i = 0\n",
    "    flux_g = sum(sum(d.g_image))\n",
    "    #return flux_i+flux_g, flux_g-flux_i\n",
    "    return flux_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(os.path.join(output_path,\"results_128_on_128\"),\"encoder_results_train_v1__all\"),'rb') as handle:\n",
    "#     encoded_imgs1=pickle.load(handle)\n",
    "#     encoded_imgs_reshaped_train = [e.reshape(-1,1) for e in encoded_imgs1]\n",
    "\n",
    "# with open(os.path.join(output_path,\"encoder_results_test_v2_\" + \"_\".join(selectedBatches)),'rb') as handle:\n",
    "#     encoded_imgs2=pickle.load(handle)\n",
    "#     encoded_imgs_reshaped_test = [e.reshape(-1,1) for e in encoded_imgs2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_encoder=load_model(os.path.join(os.path.join(output_path,\"results_128_on_128\"),\"encoder_model_v1__all\"))\n",
    "\n",
    "def get_g_Data_for_Autoencoder(data_object):\n",
    "    X_list=[(data_object[index].g_image_resized_reshaped)/data_object[index].g_image_resized_reshaped.max() for index in range(len(data_object))]\n",
    "    return np.asarray(X_list),np.asarray(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_auto_encoder,y_train_auto_encoder=get_g_Data_for_Autoencoder(batch_data_object)\n",
    "\n",
    "encoded_imgs1=model_encoder.predict(X_train_auto_encoder)\n",
    "encoded_imgs_reshaped_train = [e.reshape(-1,1) for e in encoded_imgs1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_auto_encoder,y_test_auto_encoder=get_g_Data_for_Autoencoder(batch_data_test_object)\n",
    "\n",
    "encoded_imgs2=model_encoder.predict(X_test_auto_encoder)\n",
    "encoded_imgs_reshaped_test = [e.reshape(-1,1) for e in encoded_imgs2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(659, 4100) (659,)\n"
     ]
    }
   ],
   "source": [
    "# accessible in the class: size of image, maximum of image, normalization factor of g + fitted value of normalization factor for i\n",
    "# define a function to get the training and testing dataset\n",
    "\n",
    "def obtain_train_data():\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    for i in range(len(data_train)):\n",
    "        if data_train[i].logMstar!=-99:\n",
    "            e = encoded_imgs_reshaped_train[i]\n",
    "            d = data_train[i]\n",
    "            #f1,f2 = flux(d)\n",
    "            f = flux(d)\n",
    "            size_1=d.g_image.shape[0]\n",
    "            size_2=d.g_image.shape[1]\n",
    "            img_max=d.g_image.max()\n",
    "            train_features.append(np.append(e,np.array([f,size_1,size_2,img_max])))\n",
    "            train_labels.append(d.logMstar)\n",
    "    return train_features, train_labels\n",
    "\n",
    "train_features = np.asarray(obtain_train_data()[0])\n",
    "train_labels = np.asarray(obtain_train_data()[1])\n",
    "\n",
    "print(train_features.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.589,   9.802,  10.543,  10.246,  10.861,  10.149,  10.995,\n",
       "        10.502,   8.914,  11.251,  10.712,  10.657,  10.984,   9.939,\n",
       "        11.093,   9.334,  10.625,  10.37 ,   9.104,   8.46 ,   8.586,\n",
       "         9.198,   9.231,  11.069,  10.74 ,  10.661,  10.938,  10.761,\n",
       "         9.667,  10.745,  10.313,  10.603,   8.919,  11.008,   9.728,\n",
       "         8.893,  11.195,  10.377,  10.559,  10.934,  11.279,  10.612,\n",
       "        11.004,  10.696,   9.561,  11.321,  10.302,   9.726,  10.911,\n",
       "         9.991,  10.481,   9.406,  10.13 ,  11.244,   9.162,  11.085,\n",
       "        11.088,  10.456,  10.243,   9.928,  10.155,   9.794,   9.779,\n",
       "        10.261,  10.216,  10.116,  11.086,   9.978,   9.515,  10.421,\n",
       "        11.54 ,  11.006,   9.555,   9.747,  10.796,  11.605,   9.88 ,\n",
       "        11.308,   9.245,  11.209,  10.612,  10.405,   9.35 ,  11.002,\n",
       "        11.104,   8.924,  10.674,  10.509,  11.26 ,  11.65 ,  10.896,\n",
       "        10.666,  11.056,   9.97 ,  10.285,  10.796,  10.933,  10.645,\n",
       "        10.396,  10.68 ,  10.439,  10.323,  10.234,   8.895,  10.574,\n",
       "        10.612,  10.104,  10.979,   9.989,  10.809,   9.002,  11.242,\n",
       "        10.685,  10.494,   9.754,  10.212,   9.667,   9.983,  10.529,\n",
       "         9.737,  10.727,  10.927,  10.413,  11.349,  10.636,  10.09 ,\n",
       "         8.681,   9.347,   9.289,   8.911,   8.658,   9.263,   8.985,\n",
       "         9.703,   9.796,   9.224,   9.005,   9.461,   8.943,   8.772,\n",
       "         8.633,  10.967,  10.687,   9.846,  10.246,  10.631,  10.716,\n",
       "        10.293,   9.978,  11.561,  11.309,  10.38 ,   9.101,  11.408,\n",
       "        11.579,   8.766,  10.193,  11.173,  11.453,  11.238,   8.438,\n",
       "        10.941,   9.324,  11.157,   9.869,   8.29 ,   9.841,  11.379,\n",
       "        10.712,   9.34 ,   8.442,  10.694,  10.766,  10.754,   9.52 ,\n",
       "        11.071,   9.942,  10.903,  11.365,   9.168,  10.864,  11.548,\n",
       "        11.231,   8.725,  10.892,  10.072,   9.711,  10.943,  10.79 ,\n",
       "        11.164,  10.851,   9.151,  11.261,   9.275,  10.44 ,  11.08 ,\n",
       "         9.859,  10.871,  11.004,  11.081,  10.357,  11.346,  10.742,\n",
       "        11.522,  10.861,  10.122,  10.923,  10.018,  11.24 ,   8.73 ,\n",
       "         9.424,   9.509,  11.117,  11.05 ,   9.821,   9.741,  11.116,\n",
       "        11.519,  10.933,  10.792,   9.933,  10.391,  10.073,  11.66 ,\n",
       "        11.637,  10.194,  11.28 ,  10.23 ,   9.162,  10.927,  10.017,\n",
       "        11.255,  11.34 ,  10.798,  10.104,  11.136,  11.55 ,  10.987,\n",
       "        10.593,  11.55 ,  11.542,   9.307,  10.966,  11.486,  10.725,\n",
       "        10.025,  11.208,  10.456,  10.131,   9.457,  10.293,  10.906,\n",
       "        10.65 ,  10.421,  10.382,  11.952,  10.768,  11.068,  10.282,\n",
       "         9.552,  11.108,  10.048,  10.768,  10.   ,  10.992,  10.879,\n",
       "        10.656,   9.097,  11.452,  10.7  ,  10.454,  10.956,  10.696,\n",
       "        10.983,  11.13 ,   8.964,  10.986,  10.057,   9.785,  10.502,\n",
       "         9.351,  11.035,  11.256,  10.787,  10.488,  11.085,  10.947,\n",
       "        10.567,  10.935,   9.857,  10.594,  10.529,  10.882,  10.664,\n",
       "        10.605,  10.838,  10.224,   9.862,   9.182,   9.688,   9.595,\n",
       "        10.831,  10.707,  10.726,  11.141,  10.469,  11.032,  10.805,\n",
       "        10.651,  11.243,  10.606,  10.435,  10.599,  10.725,  10.779,\n",
       "        11.145,  10.88 ,  11.047,   9.859,  10.729,   9.997,  10.764,\n",
       "        11.071,  11.01 ,  10.274,  10.601,  10.626,  10.662,  10.711,\n",
       "        11.283,  11.2  ,  10.239,  10.989,  10.575,  10.261,   9.763,\n",
       "        10.835,   9.36 ,  10.877,  11.26 ,  10.686,   9.218,  10.901,\n",
       "        10.186,  10.573,  10.839,  11.026,  10.772,  10.151,  10.2  ,\n",
       "        10.68 ,  10.794,  10.817,  11.105,  10.697,  11.171,  10.851,\n",
       "        11.133,  11.014,  11.304,  10.677,  11.574,  10.387,  10.932,\n",
       "         9.453,   9.157,  10.675,   9.831,  10.577,   9.931,  10.649,\n",
       "        10.837,  10.822,  10.365,  10.269,  10.474,  10.126,  10.01 ,\n",
       "        10.369,  10.529,  10.428,  10.322,  10.923,   9.909,   9.737,\n",
       "         9.876,  11.248,  10.034,  10.212,  11.342,  10.465,   9.518,\n",
       "        11.198,  11.024,  10.17 ,  10.208,   9.774,  10.081,  10.58 ,\n",
       "        11.376,  10.343,  10.229,  10.335,  10.073,  10.461,  11.432,\n",
       "        10.819,  11.066,  10.707,  10.148,  10.703,  11.123,   9.887,\n",
       "        10.702,  10.962,  11.018,  10.361,  10.891,  10.916,  11.036,\n",
       "        10.765,  10.719,  10.986,  10.786,   9.943,  10.509,  10.483,\n",
       "        10.323,  11.206,  10.396,  10.457,  10.432,  10.542,  10.819,\n",
       "        10.636,  11.499,  10.985,  10.443,  10.977,  10.9  ,  11.075,\n",
       "        11.107,  10.664,  10.626,  10.495,  10.01 ,   9.767,  10.835,\n",
       "        10.356,  10.628,  10.411,  10.217,  10.345,  11.045,   9.613,\n",
       "         9.918,   8.915,   9.962,  10.866,  11.485,  11.023,  10.363,\n",
       "        10.519,  10.752,  10.497,  11.155,  10.227,  11.353,  11.052,\n",
       "        11.062,  10.576,  11.476,  10.891,  10.538,   8.691,   9.704,\n",
       "         9.742,  10.088,  11.083,  10.227,   8.974,  10.797,  10.381,\n",
       "        10.818,  11.522,   9.498,  11.18 ,   9.557,   9.8  ,  10.916,\n",
       "         9.389,  10.911,  10.771,   9.369,  10.951,  11.112,   9.368,\n",
       "        10.956,   9.479,  10.666,  10.648,  11.028,  11.229,  10.967,\n",
       "        10.743,  10.294,  10.728,  10.74 ,  11.263,  10.944,  10.616,\n",
       "        10.208,  10.407,  11.079,   9.365,  10.541,  11.111,  11.559,\n",
       "        10.432,   9.368,  10.238,  10.853,  10.885,  10.335,  10.525,\n",
       "        11.022,  11.06 ,  11.381,  10.509,  10.565,  11.279,  10.692,\n",
       "        10.149,  11.127,  10.829,   9.807,  10.727,  10.489,   9.583,\n",
       "        10.541,   9.774,   8.77 ,  10.523,  11.123,  11.152,  11.004,\n",
       "         9.716,  11.013,  10.879,  10.203,  10.725,   9.992,  11.072,\n",
       "         9.599,   8.418,  10.894,  10.266,  11.118,   9.586,  11.237,\n",
       "        11.08 ,  10.874,  10.823,  10.832,  10.35 ,  11.184,  10.543,\n",
       "        10.562,   9.661,  10.838,  11.088,  11.065,  10.826,  11.336,\n",
       "        11.034,  11.39 ,  11.083,  10.811,  10.982,   9.502,   9.737,\n",
       "        10.804,   9.816,  10.71 ,  10.943,  10.674,   9.532,  10.775,\n",
       "        10.471,  10.973,  10.246,  11.211,  10.45 ,  11.106,   9.487,\n",
       "        11.051,  10.912,  10.609,  10.965,  10.994,  10.978,  10.214,\n",
       "         8.496,   9.964,  10.431,  10.673,  10.777,  10.43 ,  10.738,\n",
       "        10.405,   9.838,  11.002,   9.272,  10.755,  10.101,  10.656,\n",
       "        10.224,  11.188,   9.523,  10.492,  11.193,  11.304,  11.026,\n",
       "        11.114,  10.059,  10.309,  11.188,   9.372,  10.77 ,  11.021,\n",
       "        10.581,  10.832,   9.719,  10.682,  10.193,   9.967,   9.488,\n",
       "        10.768,   9.914,  10.584,  10.503,  11.099,  11.244,  10.357,\n",
       "         9.269,  10.879,  10.992,  10.898,  10.38 ,  11.35 ,   9.789,\n",
       "        10.666,  11.221,  11.091,  10.424,  11.321,  10.65 ,  10.772,\n",
       "        10.835])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8447, 4100) (8447,)\n"
     ]
    }
   ],
   "source": [
    "def obtain_test_data():\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    for j in range(len(data_test)):\n",
    "        if data_test[j].logMstar!=-99:\n",
    "            e = encoded_imgs_reshaped_test[i]\n",
    "            d = data_test[j]\n",
    "\n",
    "        #f1,f2 = flux(d)\n",
    "            f = flux(d)\n",
    "            size_1=d.g_image.shape[0]\n",
    "            size_2=d.g_image.shape[1]\n",
    "            img_max=d.g_image.max()\n",
    "            test_features.append(np.append(e,np.array([f,size_1,size_2,img_max])))\n",
    "            test_labels.append(d.logMstar)\n",
    "\n",
    "    return test_features, test_labels\n",
    "\n",
    "test_features = np.asarray(obtain_test_data()[0])\n",
    "test_labels = np.asarray(obtain_test_data()[1])\n",
    "\n",
    "print(test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## transform features\n",
    "# train_features = StandardScaler().fit_transform(train_features)\n",
    "# test_features = StandardScaler().transform(test_features)\n",
    "# #features = MinMaxScaler().fit_transform(features)\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "train_features_scaled = X_scaler.fit_transform(train_features)\n",
    "test_features_scaled = X_scaler.transform(test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features_scaled=train_features/(train_features.max(0).T)\n",
    "test_features_scaled=test_features/(train_features.max(0).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid', input_dim=train_features.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='Adadelta',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 2/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 3/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 4/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 5/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 6/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 7/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 8/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 9/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 10/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 11/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 12/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 13/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 14/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 15/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 16/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 17/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 18/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 19/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 20/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 21/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 22/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 23/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 24/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 25/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 26/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 27/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 28/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 29/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 30/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 31/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 32/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 33/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 34/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 35/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 36/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 37/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 38/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 39/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 40/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 41/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 42/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 43/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 44/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 45/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 46/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 47/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 48/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 49/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n",
      "Epoch 50/50\n",
      "659/659 [==============================] - 0s - loss: nan - mean_squared_error: nan     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b40e0771390>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(train_features, train_labels, epochs=100, batch_size=32,shuffle=True,\n",
    "#                 validation_data=(test_features, test_labels))\n",
    "\n",
    "model.fit(train_features_scaled, train_labels, epochs=50, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_predict = []\n",
    "#test_labels_2=[]\n",
    "for i in range(100):\n",
    "    if data_test[i].logMstar!=-99:\n",
    "        test_predict.append(model.predict(test_features_scaled[i].reshape(1,-1))[0][0])\n",
    "        #test_labels_2.append(test_labels[i])\n",
    "print(len(test_predict))\n",
    "#print(np.sqrt(mean_squared_error(np.asarray(test_predict), np.asarray(test_labels_2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_features_scaled[i].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with the entire dataset = 0.19\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "score = cross_val_score(estimator, train_features, train_labels.reshape(-1,1)).mean()\n",
    "print(\"Score with the entire dataset = %.2f\" % score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=100, n_jobs=1, oob_score=False, random_state=0,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(train_features, train_labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_predictions=estimator.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.93385,  9.93263,  9.93032,  9.93032,  9.93383,  9.89868,\n",
       "        9.93385,  9.93385,  9.95345,  9.93032])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0735f2037c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mestimator_svr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimator_svr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/leuven/319/vsc31950/miniconda3/envs/keras/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/leuven/319/vsc31950/miniconda3/envs/keras/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/data/leuven/319/vsc31950/miniconda3/envs/keras/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/leuven/319/vsc31950/miniconda3/envs/keras/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "estimator_svr=SVR()\n",
    "estimator_svr.fit(train_features_scaled, train_labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_predictions=estimator_svr.predict(test_features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.11857449,  0.        , ...,  0.10682111,\n",
       "         0.10682111,  0.01848817],\n",
       "       [ 0.03205357,  0.11624919,  0.        , ...,  0.20849421,\n",
       "         0.20849421,  0.01067172],\n",
       "       [ 0.0832845 ,  0.0904537 ,  0.14672078, ...,  0.17760618,\n",
       "         0.17760618,  0.30655937],\n",
       "       ..., \n",
       "       [ 0.        ,  0.1094537 ,  0.        , ...,  0.1023166 ,\n",
       "         0.1023166 ,  0.01492923],\n",
       "       [ 0.15812874,  0.08871009,  0.0179127 , ...,  0.12290862,\n",
       "         0.12290862,  0.01555259],\n",
       "       [ 0.09979537,  0.08998998,  0.21549305, ...,  0.09073359,\n",
       "         0.09073359,  0.0921183 ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.589,   9.802,  10.543,  10.246,  10.861,  10.149,  10.995,\n",
       "        10.502,   8.914,  11.251,  10.712,  10.657,  10.984,   9.939,\n",
       "        11.093,   9.334,  10.625,  10.37 ,   9.104,   8.46 ,   8.586,\n",
       "         9.198,   9.231,  11.069,  10.74 ,  10.661,  10.938,  10.761,\n",
       "         9.667,  10.745,  10.313,  10.603,   8.919,  11.008,   9.728,\n",
       "         8.893,  11.195,  10.377,  10.559,  10.934,  11.279,  10.612,\n",
       "        11.004,  10.696,   9.561,  11.321,  10.302,   9.726,  10.911,\n",
       "         9.991,  10.481,   9.406,  10.13 ,  11.244,   9.162,  11.085,\n",
       "        11.088,  10.456,  10.243,   9.928,  10.155,   9.794,   9.779,\n",
       "        10.261,  10.216,  10.116,  11.086,   9.978,   9.515,  10.421,\n",
       "        11.54 ,  11.006,   9.555,   9.747,  10.796,  11.605,   9.88 ,\n",
       "        11.308,   9.245,  11.209,  10.612,  10.405,   9.35 ,  11.002,\n",
       "        11.104,   8.924,  10.674,  10.509,  11.26 ,  11.65 ,  10.896,\n",
       "        10.666,  11.056,   9.97 ,  10.285,  10.796,  10.933,  10.645,\n",
       "        10.396,  10.68 ,  10.439,  10.323,  10.234,   8.895,  10.574,\n",
       "        10.612,  10.104,  10.979,   9.989,  10.809,   9.002,  11.242,\n",
       "        10.685,  10.494,   9.754,  10.212,   9.667,   9.983,  10.529,\n",
       "         9.737,  10.727,  10.927,  10.413,  11.349,  10.636,  10.09 ,\n",
       "         8.681,   9.347,   9.289,   8.911,   8.658,   9.263,   8.985,\n",
       "         9.703,   9.796,   9.224,   9.005,   9.461,   8.943,   8.772,\n",
       "         8.633,  10.967,  10.687,   9.846,  10.246,  10.631,  10.716,\n",
       "        10.293,   9.978,  11.561,  11.309,  10.38 ,   9.101,  11.408,\n",
       "        11.579,   8.766,  10.193,  11.173,  11.453,  11.238,   8.438,\n",
       "        10.941,   9.324,  11.157,   9.869,   8.29 ,   9.841,  11.379,\n",
       "        10.712,   9.34 ,   8.442,  10.694,  10.766,  10.754,   9.52 ,\n",
       "        11.071,   9.942,  10.903,  11.365,   9.168,  10.864,  11.548,\n",
       "        11.231,   8.725,  10.892,  10.072,   9.711,  10.943,  10.79 ,\n",
       "        11.164,  10.851,   9.151,  11.261,   9.275,  10.44 ,  11.08 ,\n",
       "         9.859,  10.871,  11.004,  11.081,  10.357,  11.346,  10.742,\n",
       "        11.522,  10.861,  10.122,  10.923,  10.018,  11.24 ,   8.73 ,\n",
       "         9.424,   9.509,  11.117,  11.05 ,   9.821,   9.741,  11.116,\n",
       "        11.519,  10.933,  10.792,   9.933,  10.391,  10.073,  11.66 ,\n",
       "        11.637,  10.194,  11.28 ,  10.23 ,   9.162,  10.927,  10.017,\n",
       "        11.255,  11.34 ,  10.798,  10.104,  11.136,  11.55 ,  10.987,\n",
       "        10.593,  11.55 ,  11.542,   9.307,  10.966,  11.486,  10.725,\n",
       "        10.025,  11.208,  10.456,  10.131,   9.457,  10.293,  10.906,\n",
       "        10.65 ,  10.421,  10.382,  11.952,  10.768,  11.068,  10.282,\n",
       "         9.552,  11.108,  10.048,  10.768,  10.   ,  10.992,  10.879,\n",
       "        10.656,   9.097,  11.452,  10.7  ,  10.454,  10.956,  10.696,\n",
       "        10.983,  11.13 ,   8.964,  10.986,  10.057,   9.785,  10.502,\n",
       "         9.351,  11.035,  11.256,  10.787,  10.488,  11.085,  10.947,\n",
       "        10.567,  10.935,   9.857,  10.594,  10.529,  10.882,  10.664,\n",
       "        10.605,  10.838,  10.224,   9.862,   9.182,   9.688,   9.595,\n",
       "        10.831,  10.707,  10.726,  11.141,  10.469,  11.032,  10.805,\n",
       "        10.651,  11.243,  10.606,  10.435,  10.599,  10.725,  10.779,\n",
       "        11.145,  10.88 ,  11.047,   9.859,  10.729,   9.997,  10.764,\n",
       "        11.071,  11.01 ,  10.274,  10.601,  10.626,  10.662,  10.711,\n",
       "        11.283,  11.2  ,  10.239,  10.989,  10.575,  10.261,   9.763,\n",
       "        10.835,   9.36 ,  10.877,  11.26 ,  10.686,   9.218,  10.901,\n",
       "        10.186,  10.573,  10.839,  11.026,  10.772,  10.151,  10.2  ,\n",
       "        10.68 ,  10.794,  10.817,  11.105,  10.697,  11.171,  10.851,\n",
       "        11.133,  11.014,  11.304,  10.677,  11.574,  10.387,  10.932,\n",
       "         9.453,   9.157,  10.675,   9.831,  10.577,   9.931,  10.649,\n",
       "        10.837,  10.822,  10.365,  10.269,  10.474,  10.126,  10.01 ,\n",
       "        10.369,  10.529,  10.428,  10.322,  10.923,   9.909,   9.737,\n",
       "         9.876,  11.248,  10.034,  10.212,  11.342,  10.465,   9.518,\n",
       "        11.198,  11.024,  10.17 ,  10.208,   9.774,  10.081,  10.58 ,\n",
       "        11.376,  10.343,  10.229,  10.335,  10.073,  10.461,  11.432,\n",
       "        10.819,  11.066,  10.707,  10.148,  10.703,  11.123,   9.887,\n",
       "        10.702,  10.962,  11.018,  10.361,  10.891,  10.916,  11.036,\n",
       "        10.765,  10.719,  10.986,  10.786,   9.943,  10.509,  10.483,\n",
       "        10.323,  11.206,  10.396,  10.457,  10.432,  10.542,  10.819,\n",
       "        10.636,  11.499,  10.985,  10.443,  10.977,  10.9  ,  11.075,\n",
       "        11.107,  10.664,  10.626,  10.495,  10.01 ,   9.767,  10.835,\n",
       "        10.356,  10.628,  10.411,  10.217,  10.345,  11.045,   9.613,\n",
       "         9.918,   8.915,   9.962,  10.866,  11.485,  11.023,  10.363,\n",
       "        10.519,  10.752,  10.497,  11.155,  10.227,  11.353,  11.052,\n",
       "        11.062,  10.576,  11.476,  10.891,  10.538,   8.691,   9.704,\n",
       "         9.742,  10.088,  11.083,  10.227,   8.974,  10.797,  10.381,\n",
       "        10.818,  11.522,   9.498,  11.18 ,   9.557,   9.8  ,  10.916,\n",
       "         9.389,  10.911,  10.771,   9.369,  10.951,  11.112,   9.368,\n",
       "        10.956,   9.479,  10.666,  10.648,  11.028,  11.229,  10.967,\n",
       "        10.743,  10.294,  10.728,  10.74 ,  11.263,  10.944,  10.616,\n",
       "        10.208,  10.407,  11.079,   9.365,  10.541,  11.111,  11.559,\n",
       "        10.432,   9.368,  10.238,  10.853,  10.885,  10.335,  10.525,\n",
       "        11.022,  11.06 ,  11.381,  10.509,  10.565,  11.279,  10.692,\n",
       "        10.149,  11.127,  10.829,   9.807,  10.727,  10.489,   9.583,\n",
       "        10.541,   9.774,   8.77 ,  10.523,  11.123,  11.152,  11.004,\n",
       "         9.716,  11.013,  10.879,  10.203,  10.725,   9.992,  11.072,\n",
       "         9.599,   8.418,  10.894,  10.266,  11.118,   9.586,  11.237,\n",
       "        11.08 ,  10.874,  10.823,  10.832,  10.35 ,  11.184,  10.543,\n",
       "        10.562,   9.661,  10.838,  11.088,  11.065,  10.826,  11.336,\n",
       "        11.034,  11.39 ,  11.083,  10.811,  10.982,   9.502,   9.737,\n",
       "        10.804,   9.816,  10.71 ,  10.943,  10.674,   9.532,  10.775,\n",
       "        10.471,  10.973,  10.246,  11.211,  10.45 ,  11.106,   9.487,\n",
       "        11.051,  10.912,  10.609,  10.965,  10.994,  10.978,  10.214,\n",
       "         8.496,   9.964,  10.431,  10.673,  10.777,  10.43 ,  10.738,\n",
       "        10.405,   9.838,  11.002,   9.272,  10.755,  10.101,  10.656,\n",
       "        10.224,  11.188,   9.523,  10.492,  11.193,  11.304,  11.026,\n",
       "        11.114,  10.059,  10.309,  11.188,   9.372,  10.77 ,  11.021,\n",
       "        10.581,  10.832,   9.719,  10.682,  10.193,   9.967,   9.488,\n",
       "        10.768,   9.914,  10.584,  10.503,  11.099,  11.244,  10.357,\n",
       "         9.269,  10.879,  10.992,  10.898,  10.38 ,  11.35 ,   9.789,\n",
       "        10.666,  11.221,  11.091,  10.424,  11.321,  10.65 ,  10.772,\n",
       "        10.835])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
